# deliberation/multi_ai_response.py

from __future__ import annotations

from typing import Any, Dict, Optional

import streamlit as st

from deliberation.judge_ai import JudgeAI
from components.multi_ai_judge_result_view import MultiAIJudgeResultView


class MultiAIResponse:
    """
    ãƒãƒ«ãƒAIã®å¿œç­”è¡¨ç¤º + Judge ã®çµæœè¡¨ç¤ºã‚’ã¾ã¨ã‚ã¦é¢å€’è¦‹ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    DebugPanel ã‹ã‚‰ã¯ llm_meta ã‚’ä¸¸ã”ã¨æ¸¡ã—ã¦ã‚‚ã‚‰ã†å‰æã€‚
    """

    def __init__(self, title: str = "ãƒãƒ«ãƒAIãƒ¬ã‚¹ãƒãƒ³ã‚¹") -> None:
        self.title = title
        self.judge_ai = JudgeAI()
        self.judge_view = MultiAIJudgeResultView()

    # -- llm_meta ã‹ã‚‰ models ã‚’å–ã‚Šå‡ºã™ãƒ˜ãƒ«ãƒ‘ -----------------------------

    def _extract_models(self, llm_meta: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        llm_meta["models"] ã‚’å„ªå…ˆçš„ã«è¦‹ã‚‹ã€‚
        ãã‚ŒãŒç„¡ã„å ´åˆã¯ã€æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç°¡æ˜“çš„ã«ã‚µãƒãƒ¼ãƒˆã€‚
        """
        models = llm_meta.get("models")
        if isinstance(models, dict) and models:
            return models

        # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã« gpt4o / hermes ãªã©ãŒç›´ç½®ãã•ã‚Œã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹
        candidates: Dict[str, Any] = {}
        for key, value in llm_meta.items():
            if key in {
                "route",
                "model_main",
                "usage_main",
                "usage",
                "prompt_messages",
                "prompt_preview",
                "judge",
            }:
                continue
            if isinstance(value, dict) and ("reply" in value or "text" in value):
                candidates[key] = value

        return candidates or None

    # -- ãƒ¡ã‚¤ãƒ³æç”» ---------------------------------------------------------

    def render(self, llm_meta: Optional[Dict[str, Any]]) -> None:
        st.markdown(f"#### âœï¸ {self.title}")

        if not isinstance(llm_meta, dict) or not llm_meta:
            st.caption("ï¼ˆllm_meta ãŒç©ºã®ãŸã‚ã€ãƒãƒ«ãƒAIæƒ…å ±ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ï¼‰")
            return

        # 1) ãƒ¢ãƒ‡ãƒ«å¿œç­”æ¯”è¼ƒ
        models = self._extract_models(llm_meta)
        with st.expander("ğŸ§ª ãƒ¢ãƒ‡ãƒ«å¿œç­”æ¯”è¼ƒ", expanded=True):
            if not isinstance(models, dict) or not models:
                st.caption("ï¼ˆmodels æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")
            else:
                for key, data in models.items():
                    if not isinstance(data, dict):
                        continue

                    reply = data.get("reply") or data.get("text") or ""
                    model_name = data.get("model_name") or key
                    route = data.get("route") or llm_meta.get("route") or "unknown"

                    st.markdown(f"**{model_name}**  (_{key}_, route: `{route}`)")
                    if reply:
                        st.write(reply)
                    else:
                        st.caption("ï¼ˆè¿”ä¿¡ãƒ†ã‚­ã‚¹ãƒˆãªã—ï¼‰")

                    usage = data.get("usage") or data.get("usage_main")
                    if isinstance(usage, dict):
                        pt = usage.get("prompt_tokens", "ï¼Ÿ")
                        ct = usage.get("completion_tokens", "ï¼Ÿ")
                        tt = usage.get("total_tokens", "ï¼Ÿ")
                        st.caption(
                            f"tokens: total={tt}, prompt={pt}, completion={ct}"
                        )

                    st.markdown("---")

        # 2) Judge å®Ÿè¡Œï¼†çµæœè¡¨ç¤º
        judge = llm_meta.get("judge")
        if not isinstance(judge, dict):
            # å¿…è¦ã§ã‚ã‚Œã°æ–°ãŸã«å¯©è­°ã‚’å®Ÿè¡Œ
            judge = self.judge_ai.run(llm_meta)

        with st.expander("âš–ï¸ ãƒãƒ«ãƒAIå¯©è­°çµæœ", expanded=True):
            self.judge_view.render(judge)
